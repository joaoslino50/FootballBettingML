\documentclass[conference]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{multirow}
\usepackage{array}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\title{Machine Learning for Football Betting:\\Predictions with Uncertainty Quantification and Knowledge Graphs}


\author{
\IEEEauthorblockN{João Lino}
\IEEEauthorblockA{Knowledge and Learning\\
Departamento de Engenharia Informática da Universidade de Coimbra\\
Coimbra, Portugal\\
uc2025246120@student.uc.pt}
}

\begin{document}

\maketitle

\begin{abstract}
This paper presents a comprehensive knowledge-based system for football betting predictions that integrates machine learning models with semantic web technologies. The system employs ensemble methods including Random Forest and Gradient Boosting classifiers with probability calibration to predict multiple betting markets: match results, over/under 2.5 goals, both teams to score (BTTS), and clean sheets. A key innovation is the integration of uncertainty quantification using bootstrap methods, providing confidence intervals for all predictions. The knowledge representation layer utilizes RDF graphs with Schema.org vocabulary and custom OWL ontology, enabling SPARQL-based feature extraction and semantic queries. An interactive chatbot interface powered by LangChain and Ollama provides natural language access to predictions and statistics, while a dedicated odds analyzer calculates expected value to identify value betting opportunities. Experimental results on over 80,000 historical matches demonstrate model accuracies of 61\% for match results, 75\% for over/under 2.5 goals, 74\% for BTTS, and 78-82\% for clean sheet predictions.
\end{abstract}

\begin{IEEEkeywords}
Machine Learning, Knowledge Graphs, Football Betting, Uncertainty Quantification, Semantic Web, Natural Language Processing
\end{IEEEkeywords}

\section{Introduction}

\subsection{Motivation}

The sports betting industry has experienced exponential growth in recent years, with global revenues exceeding \$200 billion annually. Football (soccer), being the world's most popular sport, accounts for a significant portion of this market. Despite the vast amounts of data generated by football matches, most bettors rely on intuition or simple statistics, often leading to suboptimal decisions. The integration of machine learning with knowledge representation techniques presents an opportunity to develop more sophisticated prediction systems that can identify value betting opportunities.

\subsection{Problem Description}

Traditional approaches to football betting prediction face several challenges:
\begin{itemize}
    \item \textbf{Data Integration}: Historical match data exists in various formats and sources, making comprehensive analysis difficult.
    \item \textbf{Uncertainty}: Point predictions without confidence intervals provide incomplete information for betting decisions.
    \item \textbf{Knowledge Representation}: Relationships between teams, matches, and betting markets are not explicitly modeled.
    \item \textbf{Accessibility}: Complex ML predictions are difficult for average users to interpret and utilize.
\end{itemize}

\subsection{Goals and Approach}

This project aims to develop a comprehensive football betting prediction system that addresses these challenges through:

\begin{enumerate}
    \item \textbf{Multi-market Prediction}: ML models for match results (1X2), over/under 2.5 goals, BTTS, and clean sheets.
    \item \textbf{Uncertainty Quantification}: Bootstrap-based confidence intervals for all predictions.
    \item \textbf{Knowledge Graph Integration}: RDF-based semantic representation using Schema.org and custom OWL ontology.
    \item \textbf{Feature Engineering}: Elo ratings, form indicators, head-to-head statistics, and goal efficiency metrics.
    \item \textbf{User Interface}: Interactive chatbot with NLP-based query parsing and dedicated odds analyzer.
\end{enumerate}

\section{Related Work}

\subsection{Machine Learning in Sports Prediction}

Machine learning approaches for football prediction have been extensively studied. Constantinou et al. \cite{constantinou2012} demonstrated that Bayesian networks could outperform betting market predictions in certain scenarios. More recently, Hubáček et al. \cite{hubacek2019} achieved competitive results using gradient boosting methods with carefully engineered features.

Elo rating systems, originally developed for chess, have been adapted for football prediction with considerable success \cite{hvattum2010}. These ratings capture team strength dynamics and provide interpretable features for ML models.

\subsection{Uncertainty Quantification in ML}

The importance of uncertainty estimation in machine learning has been recognized across domains. Gal and Ghahramani \cite{gal2016} introduced dropout as a Bayesian approximation for uncertainty estimation. For classification tasks, probability calibration methods such as Platt scaling and isotonic regression have been shown to improve prediction reliability \cite{niculescu2005}.

The UQ360 toolkit \cite{ghosh2021} provides comprehensive uncertainty quantification methods, though compatibility challenges with recent Python versions necessitate alternative approaches such as bootstrap sampling.

\subsection{Knowledge Graphs in Sports Analytics}

Knowledge graphs have emerged as powerful tools for representing complex domain knowledge. The use of Schema.org vocabulary provides standardization for sports-related entities \cite{guha2016}. SPARQL queries enable complex reasoning over graph structures, facilitating feature extraction and semantic search capabilities \cite{harris2013}.

\subsection{Chatbots and NLP for Data Access}

Large language models combined with tool-calling capabilities have revolutionized data access interfaces. LangChain \cite{langchain2023} provides abstractions for building LLM-powered applications with structured tool access. SpaCy \cite{honnibal2020} offers efficient named entity recognition that can be adapted for domain-specific entity extraction.

\section{Data \& Approach}

\subsection{Dataset Description}

The system utilizes historical football match data from multiple European leagues. While the raw dataset spans from 1993 to 2025, the models are trained exclusively on data from 2015 onwards to ensure feature consistency and data quality. This filtered dataset comprises over 80,000 matches and includes:

\begin{itemize}
    \item Match information: date, home team, away team, division
    \item Results: full-time home goals (FTHG), full-time away goals (FTAG)
    \item Betting odds: home win, draw, away win odds from multiple bookmakers
    \item Derived targets: match result, over/under 2.5 goals, BTTS, clean sheets
\end{itemize}

Table \ref{tab:data_stats} summarizes the dataset characteristics.

\begin{table}[h]
\centering
\caption{Dataset Statistics}
\label{tab:data_stats}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Total Matches (2015+) & 82,847 \\
Unique Teams & 552 \\
Leagues/Divisions & 23 \\
Training Date Range & 2015-2025 \\
Home Win Rate & 45.8\% \\
Draw Rate & 26.2\% \\
Away Win Rate & 28.0\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{System Architecture}

The system architecture (Fig. \ref{fig:architecture}) comprises four main layers:

\begin{enumerate}
    \item \textbf{Data Layer}: Raw CSV data preprocessing and cleaning
    \item \textbf{Feature Engineering Layer}: Elo ratings, form, head-to-head, goal metrics
    \item \textbf{ML Layer}: Ensemble models with calibration and uncertainty
    \item \textbf{Knowledge Layer}: RDF graph, OWL ontology, SPARQL queries
    \item \textbf{Interface Layer}: Streamlit UI, chatbot, odds analyzer
\end{enumerate}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/system-architecture.jpg}
\caption{System architecture showing the data flow from user interface through NLP preprocessing, knowledge graph, ML models, feature engineering, and data preprocessing layers.}
\label{fig:architecture}
\end{figure}

\subsection{Feature Engineering}

The feature engineering pipeline generates several categories of predictive features:

\textbf{Elo Ratings}: Dynamic team strength ratings updated after each match using the formula:
\begin{equation}
E_{new} = E_{old} + K \cdot (S - E[S])
\end{equation}
where $K=32$ is the update factor, $S$ is the actual result (1/0.5/0), and $E[S]$ is the expected score based on rating difference.

\textbf{Form Indicators}: Rolling averages over the last 3 and 5 matches for points, goals scored, and goals conceded, calculated separately for home and away performances.

\textbf{Head-to-Head Statistics}: Historical win rates and average goals in previous encounters between the same teams.

\textbf{Goal Efficiency}: Ratio of goals scored to expected goals based on team averages, capturing over/under-performance.

\section{Implementation}

\subsection{Machine Learning Pipeline}

\textbf{Data Preprocessing}: The raw dataset is filtered to include only matches from 2015 onwards to ensure consistent feature availability (particularly betting odds and Elo ratings) and to focus on more recent football dynamics. This temporal filtering is applied before feature engineering to maintain data quality.

The ML pipeline employs ensemble methods with probability calibration:

\begin{lstlisting}[language=Python, caption=Model Training Pipeline]
# Base models
rf = RandomForestClassifier(
    n_estimators=200,
    max_depth=15,
    min_samples_split=10
)
gb = GradientBoostingClassifier(
    n_estimators=150,
    max_depth=5,
    learning_rate=0.1
)

# Probability calibration
calibrated = CalibratedClassifierCV(
    base_estimator,
    method='isotonic',
    cv=5
)
\end{lstlisting}

\textbf{Uncertainty Quantification}: Bootstrap sampling generates prediction distributions:

\begin{lstlisting}[language=Python, caption=Bootstrap Uncertainty]
def predict_with_uncertainty(X, n_bootstrap=100):
    predictions = []
    for _ in range(n_bootstrap):
        idx = np.random.choice(len(X_train), 
                               size=len(X_train))
        model.fit(X_train[idx], y_train[idx])
        predictions.append(model.predict_proba(X))
    
    mean = np.mean(predictions, axis=0)
    lower = np.percentile(predictions, 5, axis=0)
    upper = np.percentile(predictions, 95, axis=0)
    return mean, lower, upper
\end{lstlisting}

\subsection{Knowledge Graph Implementation}

The knowledge graph uses RDFLib with Schema.org vocabulary extended by a custom ontology:

\begin{lstlisting}[language=Python, caption=Knowledge Graph Construction]
SCHEMA = Namespace("http://schema.org/")
STRIKO = Namespace("http://striko.football/betting/")

# Add match as SportsEvent
g.add((match_uri, RDF.type, SCHEMA.SportsEvent))
g.add((match_uri, SCHEMA.homeTeam, home_uri))
g.add((match_uri, SCHEMA.awayTeam, away_uri))
g.add((match_uri, STRIKO.homeElo, 
       Literal(elo, datatype=XSD.float)))

# Add prediction with uncertainty
g.add((pred_uri, STRIKO.probability, 
       Literal(prob, datatype=XSD.float)))
g.add((pred_uri, STRIKO.lowerBound, 
       Literal(lower, datatype=XSD.float)))
g.add((pred_uri, STRIKO.upperBound, 
       Literal(upper, datatype=XSD.float)))
\end{lstlisting}

The OWL ontology defines classes for \texttt{BettingOdds}, \texttt{Prediction}, and \texttt{UncertaintyInterval} with appropriate object and data properties.

\subsection{SPARQL Feature Extraction}

Features can be extracted directly from the knowledge graph:

\begin{lstlisting}[language=SPARQL, caption=Elo Feature Query]
SELECT ?homeElo ?awayElo WHERE {
    ?match schema:homeTeam ?home .
    ?match schema:awayTeam ?away .
    ?home schema:name "Liverpool" .
    ?away schema:name "Arsenal" .
    ?match striko:homeElo ?homeElo .
    ?match striko:awayElo ?awayElo .
} ORDER BY DESC(?date) LIMIT 1
\end{lstlisting}

\subsection{NLP and Chatbot Interface}

The chatbot uses spaCy for entity extraction and intent classification:

\begin{lstlisting}[language=Python, caption=NLP Query Preprocessing]
class NLPPreprocessor:
    def __init__(self):
        self.nlp = spacy.load("en_core_web_sm")
        self.team_names = load_team_names()
    
    def extract_teams(self, text):
        # Fuzzy matching against known teams
        matches = []
        for team in self.team_names:
            if fuzz.ratio(team.lower(), 
                         text.lower()) > 80:
                matches.append(team)
        return matches
    
    def classify_intent(self, text):
        # Intent classification based on keywords
        if any(w in text for w in 
               ['predict', 'win', 'winner']):
            return 'prediction'
        elif any(w in text for w in 
                 ['stats', 'statistics', 'history']):
            return 'statistics'
        return 'general'
\end{lstlisting}

LangChain orchestrates tool calling with the Ollama LLM:

\begin{lstlisting}[language=Python, caption=LangChain Agent Setup]
tools = [
    get_team_statistics,
    get_team_matches,
    predict_match_result,
    get_overall_statistics
]

agent = create_tool_calling_agent(llm, tools, prompt)
executor = AgentExecutor(agent=agent, tools=tools)
\end{lstlisting}

\subsection{Odds Analyzer}

The odds analyzer calculates expected value (EV) for betting decisions:

\begin{equation}
EV = (P_{model} \times Odds) - 1
\end{equation}

where $P_{model}$ is the model's predicted probability and $Odds$ are the decimal bookmaker odds. Positive EV indicates a value betting opportunity.

\section{Experimentation}

\subsection{Model Performance}

Table \ref{tab:model_performance} and Figure \ref{fig:model_perf} present the performance metrics for each prediction market.

\begin{table}[h]
\centering
\caption{Model Performance Metrics}
\label{tab:model_performance}
\begin{tabular}{lcccc}
\toprule
\textbf{Market} & \textbf{Accuracy} & \textbf{AUC} & \textbf{Log Loss} \\
\midrule
Match Result & 61.2\% & 0.78 & 0.89 \\
Over/Under 2.5 & 74.8\% & 0.82 & 0.51 \\
BTTS & 73.6\% & 0.80 & 0.53 \\
Clean Sheet Home & 81.7\% & 0.75 & 0.42 \\
Clean Sheet Away & 78.2\% & 0.73 & 0.45 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/model_performance.pdf}
\caption{Comparison of model accuracy and AUC scores across betting markets. Binary prediction markets (Over/Under, BTTS, Clean Sheets) achieve higher accuracy than the three-class match result prediction.}
\label{fig:model_perf}
\end{figure}

\subsection{Calibration Analysis}

Probability calibration significantly improved prediction reliability. Figure \ref{fig:calibration} illustrates the improvement in calibration curves after applying isotonic regression. Table \ref{tab:calibration} quantifies the reduction in Brier score.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/calibration_curves.pdf}
\caption{Calibration curves before (left) and after (right) isotonic regression. The dashed line represents perfect calibration.}
\label{fig:calibration}
\end{figure}

\begin{table}[h]
\centering
\caption{Calibration Error (Brier Score)}
\label{tab:calibration}
\begin{tabular}{lcc}
\toprule
\textbf{Market} & \textbf{Before} & \textbf{After} \\
\midrule
Match Result & 0.234 & 0.198 \\
Over/Under 2.5 & 0.187 & 0.162 \\
BTTS & 0.192 & 0.171 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{User Interface}

The system provides an interactive web interface built with Streamlit, featuring two main components: an AI-powered chatbot for natural language queries and a dedicated odds analyzer for value betting identification. Figure \ref{fig:ui_screenshots} shows screenshots of the odds analyzer interface in action.

\begin{figure*}[t]
\centering
\begin{tabular}{cc}
\includegraphics[width=0.48\textwidth]{figures/prediction-game-striko.jpg} &
\includegraphics[width=0.48\textwidth]{figures/prediction-game-flashscore.jpg} \\
(a) System Prediction Interface & (b) Real Match Result \\
\end{tabular}
\caption{User interface comparison: (a) Our system's odds analyzer showing predictions for Aston Villa vs Arsenal with expected value calculations and betting recommendations, (b) Actual match result from FlashScore for reference. The interface displays model probabilities, bookmaker odds, expected value (EV), and clear betting recommendations (STRONG BET, DON'T BET) with color-coded indicators.}
\label{fig:ui_screenshots}
\end{figure*}

The odds analyzer interface (Figure \ref{fig:ui_screenshots}a) demonstrates the system's value betting analysis capabilities. For the Aston Villa vs Arsenal match, the model predicted:
\begin{itemize}
    \item Home Win (Aston Villa): 45.7\% probability, EV: +94.4\% at odds 4.25
    \item Draw: 31.5\% probability, EV: +11.8\% at odds 3.55
    \item Away Win (Arsenal): 22.8\% probability, EV: -59.5\% at odds 1.78
\end{itemize}

The system correctly identified Aston Villa as the best value bet, recommending a STRONG BET with +94.4\% expected value. This demonstrates how the integration of ML predictions with bookmaker odds enables users to identify potentially profitable betting opportunities.

\subsection{Knowledge Graph Statistics}

The generated knowledge graph contains:
\begin{itemize}
    \item 126,040 RDF triples
    \item 82,847 match entities
    \item 552 team entities
    \item 414,235 prediction entities (5 markets × matches)
\end{itemize}

SPARQL query response times average 45ms for simple queries and 320ms for complex aggregations.

\subsection{Value Betting Analysis}

Analysis of historical data shows that betting on positive EV opportunities with EV $>$ 5\% yields a theoretical return of 3.2\% over baseline. However, variance remains high due to the inherent uncertainty in football outcomes.

\begin{table}[h]
\centering
\caption{Value Betting Simulation Results}
\label{tab:value_betting}
\begin{tabular}{lcc}
\toprule
\textbf{EV Threshold} & \textbf{Bets} & \textbf{ROI} \\
\midrule
$>$ 0\% & 12,453 & +0.8\% \\
$>$ 5\% & 4,231 & +3.2\% \\
$>$ 10\% & 1,876 & +5.1\% \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusions and Future Work}

\subsection{Main Achievements}

This project successfully developed a comprehensive football betting prediction system that integrates:

\begin{enumerate}
    \item \textbf{Accurate ML Models}: Achieved 61-82\% accuracy across different betting markets, with well-calibrated probability estimates.
    \item \textbf{Uncertainty Quantification}: Bootstrap-based confidence intervals provide reliable uncertainty estimates, crucial for betting decisions.
    \item \textbf{Knowledge Graph}: RDF-based semantic representation with 126,000+ triples enables complex queries and reasoning.
    \item \textbf{User-Friendly Interface}: Interactive chatbot and odds analyzer make predictions accessible to non-technical users.
    \item \textbf{Value Identification}: Expected value calculations help identify potentially profitable betting opportunities.
\end{enumerate}

\subsection{Main Difficulties}

Several challenges were encountered during development:

\begin{itemize}
    \item \textbf{UQ360 Compatibility}: The UQ360 library had compatibility issues with Python 3.12, requiring implementation of alternative bootstrap methods.
    \item \textbf{LangChain API Changes}: Rapid evolution of the LangChain library necessitated multiple code updates to maintain compatibility.
    \item \textbf{Feature Engineering Complexity}: Calculating dynamic features like Elo ratings required careful handling of temporal dependencies to avoid data leakage.
    \item \textbf{Class Imbalance}: The three-class match result prediction suffered from imbalanced classes, particularly the draw class.
\end{itemize}

\subsection{Future Work}

Several directions for future improvement are identified:

\begin{enumerate}
    \item \textbf{Deep Learning Models}: Explore LSTM or Transformer architectures for capturing temporal patterns in team performance.
    \item \textbf{Additional Data Sources}: Integrate player-level statistics, injury reports, and weather data.
    \item \textbf{Real-time Updates}: Implement live data feeds for up-to-date predictions.
    \item \textbf{Ontology Expansion}: Extend the OWL ontology to capture more complex relationships and enable SWRL rule-based reasoning.
    \item \textbf{Bankroll Management}: Add Kelly criterion-based stake recommendations.
    \item \textbf{Model Explanations}: Integrate SHAP or LIME for interpretable predictions.
\end{enumerate}

\section*{Acknowledgments}

This work was developed as part of the Knowledge and Learning course at FEUP. The author thanks the course instructors for their guidance and feedback throughout the project.

\bibliographystyle{IEEEtran}
\begin{thebibliography}{10}

\bibitem{constantinou2012}
A. C. Constantinou, N. E. Fenton, and M. Neil, ``pi-football: A Bayesian network model for forecasting Association Football match outcomes,'' \textit{Knowledge-Based Systems}, vol. 36, pp. 322--339, 2012.

\bibitem{hubacek2019}
O. Hubáček, G. Šourek, and F. Železný, ``Exploiting sports-betting market using machine learning,'' \textit{International Journal of Forecasting}, vol. 35, no. 2, pp. 783--796, 2019.

\bibitem{hvattum2010}
L. M. Hvattum and H. Arntzen, ``Using ELO ratings for match result prediction in association football,'' \textit{International Journal of Forecasting}, vol. 26, no. 3, pp. 460--470, 2010.

\bibitem{gal2016}
Y. Gal and Z. Ghahramani, ``Dropout as a Bayesian approximation: Representing model uncertainty in deep learning,'' in \textit{Proc. ICML}, 2016, pp. 1050--1059.

\bibitem{niculescu2005}
A. Niculescu-Mizil and R. Caruana, ``Predicting good probabilities with supervised learning,'' in \textit{Proc. ICML}, 2005, pp. 625--632.

\bibitem{ghosh2021}
S. Ghosh et al., ``Uncertainty quantification 360: A holistic toolkit for quantifying and communicating the uncertainty of AI,'' \textit{arXiv preprint arXiv:2106.01410}, 2021.

\bibitem{guha2016}
R. V. Guha, D. Brickley, and S. Macbeth, ``Schema.org: Evolution of structured data on the web,'' \textit{Communications of the ACM}, vol. 59, no. 2, pp. 44--51, 2016.

\bibitem{harris2013}
S. Harris and A. Seaborne, ``SPARQL 1.1 query language,'' \textit{W3C Recommendation}, 2013.

\bibitem{langchain2023}
LangChain, ``LangChain: Building applications with LLMs through composability,'' 2023. [Online]. Available: https://langchain.com

\bibitem{honnibal2020}
M. Honnibal, I. Montani, S. Van Landeghem, and A. Boyd, ``spaCy: Industrial-strength natural language processing in Python,'' 2020.

\end{thebibliography}

\end{document}

